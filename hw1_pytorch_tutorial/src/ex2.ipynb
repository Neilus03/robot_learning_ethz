{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfac88e5",
   "metadata": {},
   "source": [
    "# Exercise 2: PyTorch core\n",
    "\n",
    "In this exercise you’ll build core PyTorch “muscle memory” that you’ll reuse in basically every model you write:\n",
    "\n",
    "- **Autograd**: how gradients are created, how they accumulate, and how to compute gradients for one or multiple inputs.\n",
    "- **Dataloading**: writing small `Dataset`s, using `DataLoader`, and custom `collate_fn`.\n",
    "- **Optimizers**: implementing **AdamW** updates from scratch (state, bias correction, weight decay).\n",
    "- **Training basics**: a clean single training step.\n",
    "- **Initialization**: fan-in/out and common initializers (Xavier / Kaiming), plus a helper to init `nn.Linear`.\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "When debugging, print shapes/dtypes/devices, and write tiny sanity checks (e.g. compare to PyTorch’s built-ins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0145b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neild\\Miniconda3\\envs\\robot_learning_env\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:283: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419608fc",
   "metadata": {},
   "source": [
    "## Autograd fundamentals\n",
    "\n",
    "PyTorch builds a computation graph when you apply operations to tensors with `requires_grad=True`.\n",
    "Calling `backward()` (or `torch.autograd.grad`) computes gradients by traversing that graph.\n",
    "\n",
    "### Key concepts\n",
    "- **Leaf tensor**: a tensor created by you (not the result of an operation) with `requires_grad=True`. Leaf tensors can store gradients in `.grad`.\n",
    "- **Gradient accumulation**: calling `backward()` adds into `.grad` (it does not overwrite). You must reset gradients between steps/calls.\n",
    "- **`torch.autograd.grad` vs `.backward()`**\n",
    "  - `torch.autograd.grad(f, x)` returns `df/dx` directly and does not write into `x.grad` unless you explicitly do so.\n",
    "  - `f.backward()` writes gradients into `.grad` of leaf tensors.\n",
    "\n",
    "In the next functions you’ll compute gradients for a simple scalar function such as `f(x) = sum(x^2)` using both APIs.\n",
    "\n",
    "### `torch.no_grad()`\n",
    "Wrap inference-only code to avoid tracking gradients and building graphs:\n",
    "- saves memory\n",
    "- speeds up evaluation\n",
    "\n",
    "### `detach()`\n",
    "`y = x.detach()` returns a tensor that shares data with `x` but is **not connected** to the autograd graph.\n",
    "This is useful when you want to treat something as a constant target.\n",
    "\n",
    "### `model.train()` vs `model.eval()`\n",
    "- `train()` enables training behavior (e.g. dropout active, batchnorm updates running stats).\n",
    "- `eval()` enables inference behavior (e.g. dropout off, batchnorm uses running stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7724fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing grad_with_autograd_grad:\n",
      "Input x: tensor([1., 2., 3.], requires_grad=True)\n",
      "Gradient df/dx: tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n",
      "Expected: tensor([2., 4., 6.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def grad_with_autograd_grad(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using torch.autograd.grad\n",
    "\n",
    "    Requirements:\n",
    "    - Do not call .backward().\n",
    "    - x should require grad inside the function (don't assume it does).\n",
    "    - Must return df/dx\n",
    "    \"\"\"\n",
    "    # Ensure x requires grad\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "    # Define the function f(x) = sum(x^2)\n",
    "    f = torch.sum(x ** 2)\n",
    "\n",
    "    # Compute the gradient df/dx using torch.autograd.grad\n",
    "    grad = torch.autograd.grad(f, x, create_graph=True)[0]\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Test grad_with_autograd_grad\n",
    "x_test = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "grad_result = grad_with_autograd_grad(x_test)\n",
    "print(\"Testing grad_with_autograd_grad:\")\n",
    "print(f\"Input x: {x_test}\")\n",
    "print(f\"Gradient df/dx: {grad_result}\")\n",
    "print(f\"Expected: {2 * x_test.detach()}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0d32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing grad_with_backward:\n",
      "Input x: tensor([1., 2., 3.], requires_grad=True)\n",
      "Gradient df/dx: tensor([2., 4., 6.])\n",
      "Expected: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def grad_with_backward(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using .backward().\n",
    "\n",
    "    Requirements:\n",
    "    - Must return df/dx\n",
    "    - Must not leak gradients across calls (watch x.grad accumulation)\n",
    "    \"\"\"\n",
    "    # Ensure x requires grad\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "    # Define the function f(x) = sum(x^2)\n",
    "    f = torch.sum(x ** 2)\n",
    "\n",
    "    # Compute the gradient df/dx using torch.backward()\n",
    "    f.backward()\n",
    "    # Get the gradient from x.grad\n",
    "    grad = x.grad.clone()  # Clone to avoid in-place modification\n",
    "    # Zero out the gradients to prevent accumulation\n",
    "    x.grad.zero_()\n",
    "    return grad\n",
    "\n",
    "# Test grad_with_backward\n",
    "x_test = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "grad_result = grad_with_backward(x_test)\n",
    "print(\"Testing grad_with_backward:\")\n",
    "print(f\"Input x: {x_test}\")\n",
    "print(f\"Gradient df/dx: {grad_result}\")\n",
    "print(f\"Expected: {2 * x_test.detach()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec40995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing grad_wrt_multiple_inputs:\n",
      "Input a: tensor([1., 2., 3.], requires_grad=True)\n",
      "Input b: tensor([0.5000, 1.0000, 1.5000], requires_grad=True)\n",
      "\n",
      "Gradient df/da: tensor([2.5000, 5.0000, 7.5000], grad_fn=<AddBackward0>)\n",
      "Gradient df/db: tensor([1., 2., 3.], grad_fn=<MulBackward0>)\n",
      "\n",
      "Expected df/da: tensor([2.5000, 5.0000, 7.5000])\n",
      "Expected df/db: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "def grad_wrt_multiple_inputs(\n",
    "    a: torch.Tensor, b: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute gradients w.r.t. multiple inputs. The function is f(a, b) = sum(a^2 + ab).\n",
    "\n",
    "    Return:\n",
    "        (df/da, df/db)\n",
    "\n",
    "    Requirements:\n",
    "    - Use torch.autograd.grad\n",
    "    - Ensure both a and b require grad in this function.\n",
    "    \"\"\"\n",
    "    # Ensure both a and b require grad\n",
    "    if not a.requires_grad:\n",
    "        a.requires_grad_(True)\n",
    "    if not b.requires_grad:\n",
    "        b.requires_grad_(True)\n",
    "\n",
    "    # Define the function f(a, b) = sum(a^2 + ab)\n",
    "    f = torch.sum(a ** 2 + a * b)\n",
    "\n",
    "    # Compute the gradients df/da and df/db using torch.autograd.grad\n",
    "    grad_a, grad_b = torch.autograd.grad(f, (a, b), create_graph=True)\n",
    "\n",
    "    return grad_a, grad_b\n",
    "\n",
    "\n",
    "# Test grad_wrt_multiple_inputs\n",
    "a_test = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b_test = torch.tensor([0.5, 1.0, 1.5], requires_grad=True)\n",
    "\n",
    "grad_result = grad_wrt_multiple_inputs(a_test, b_test)\n",
    "print(\"Testing grad_wrt_multiple_inputs:\")\n",
    "print(f\"Input a: {a_test}\")\n",
    "print(f\"Input b: {b_test}\")\n",
    "print()\n",
    "print(f\"Gradient df/da: {grad_result[0]}\")\n",
    "print(f\"Gradient df/db: {grad_result[1]}\")\n",
    "print()\n",
    "print(f\"Expected df/da: {2 * a_test.detach() + b_test.detach()}\")\n",
    "print(f\"Expected df/db: {a_test.detach()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a026b",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "In PyTorch, a `Dataset` defines how to fetch a *single* training example, and a `DataLoader` handles:\n",
    "- batching\n",
    "- shuffling\n",
    "- parallel workers\n",
    "- optional custom batching logic via `collate_fn`\n",
    "\n",
    "### `Dataset` in one sentence\n",
    "A `Dataset` only needs:\n",
    "- `__len__`: number of items\n",
    "- `__getitem__`: return one item (e.g. `(x, y)`)\n",
    "\n",
    "### Why `collate_fn` matters\n",
    "The default DataLoader collation stacks items along a new batch dimension.\n",
    "That works for fixed-size tensors, but it breaks for **variable-length sequences**.\n",
    "\n",
    "So we’ll implement padding ourselves:\n",
    "- Convert a list of 1D token sequences into a padded tensor `(B, T_max)`\n",
    "- Track `lengths` and a `padding_mask`\n",
    "\n",
    "### Mask convention for padding\n",
    "For padding masks in this exercise:\n",
    "- `padding_mask[b, t] == True` means **this is padding / invalid**\n",
    "- `padding_mask[b, t] == False` means **this is a real token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbd7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09ab0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset wrapping (x, y).\n",
    "\n",
    "    x: (N, ...)\n",
    "    y: (N, ...)\n",
    "\n",
    "    N is the number of samples. The dataset should return tuples of (x[i], y[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        assert len(x) == len(y), \"x and y must have the same number of samples\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e02fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NextTokenDataset:\n",
      "Sample 0:\n",
      "Input IDs: tensor([1, 2, 3, 4, 5])\n",
      "Target IDs: tensor([2, 3, 4, 5, 6])\n",
      "\n",
      "Sample 1:\n",
      "Input IDs: tensor([ 7,  8,  9, 10, 11])\n",
      "Target IDs: tensor([ 8,  9, 10, 11, 12])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NextTokenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Next-token prediction dataset.\n",
    "\n",
    "    Given tokens of shape (N, T), produce:\n",
    "      input_ids  = tokens[:, :-1]\n",
    "      target_ids = tokens[:, 1:]\n",
    "\n",
    "    Return per item:\n",
    "      (input_ids, target_ids)\n",
    "\n",
    "    Notes:\n",
    "    - Returned tensors should be 1D of length (T-1).\n",
    "    - dtype should remain integer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor):\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        sequence = self.tokens[idx]\n",
    "        input_ids = sequence[:-1]\n",
    "        target_ids = sequence[1:]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# Test NextTokenDataset\n",
    "tokens_test = torch.tensor([[1,2,3,4,5,6], [7,8,9,10,11,12]])\n",
    "dataset = NextTokenDataset(tokens_test)\n",
    "print(\"Testing NextTokenDataset:\")\n",
    "for i in range(len(dataset)):\n",
    "    input_ids, target_ids = dataset[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Input IDs: {input_ids}\")\n",
    "    print(f\"Target IDs: {target_ids}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RandomCropSequenceDataset:\n",
      "Sample 0: Crop: tensor([3, 4, 5])\n",
      "Sample 1: Crop: tensor([10, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RandomCropSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sequence dataset that returns random crops of fixed length.\n",
    "\n",
    "    tokens: (N, T_total)\n",
    "    crop_len: L\n",
    "\n",
    "    For each __getitem__:\n",
    "      - sample a start index s so that s+L <= T_total\n",
    "      - return tokens[idx, s:s+L]\n",
    "\n",
    "    Requirements:\n",
    "    - Use a torch.Generator for deterministic behavior if seed is provided.\n",
    "    - Do NOT use Python's random module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor, crop_len: int, seed: int | None = None):\n",
    "        self.tokens = tokens\n",
    "        self.crop_len = crop_len\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None:\n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        sequence = self.tokens[idx]\n",
    "        T_total = sequence.size(0)\n",
    "        if T_total <= self.crop_len:\n",
    "            raise ValueError(f\"Crop length {self.crop_len} must be less than sequence length {T_total}\")\n",
    "        max_start = T_total - self.crop_len\n",
    "        start_idx = torch.randint(0, max_start + 1, (1,), generator=self.generator).item()\n",
    "        crop = sequence[start_idx:start_idx + self.crop_len]\n",
    "        return crop\n",
    "\n",
    "# Test RandomCropSequenceDataset\n",
    "tokens_test = torch.tensor([[1,2,3,4,5,6], [7,8,9,10,11,12]])\n",
    "crop_len = 3\n",
    "dataset = RandomCropSequenceDataset(tokens_test, crop_len, seed=42)\n",
    "print(\"Testing RandomCropSequenceDataset:\")\n",
    "for i in range(len(dataset)):\n",
    "    crop = dataset[i]\n",
    "    print(f\"Sample {i}: Crop: {crop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73593f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pad_1d_sequences:\n",
      "Tokens:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 0],\n",
      "        [6, 0, 0]])\n",
      "Lengths:\n",
      "tensor([3, 2, 1])\n",
      "Padding Mask:\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PaddedBatch:\n",
    "    \"\"\"\n",
    "    A padded batch for variable-length sequences.\n",
    "\n",
    "    tokens: LongTensor (B, T_max)\n",
    "    lengths: LongTensor (B,)\n",
    "    padding_mask: BoolTensor (B, T_max) where True means \"this is padding\"\n",
    "    \"\"\"\n",
    "\n",
    "    tokens: torch.Tensor\n",
    "    lengths: torch.Tensor\n",
    "    padding_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def pad_1d_sequences(seqs: list[torch.Tensor], pad_value: int = 0) -> PaddedBatch:\n",
    "    \"\"\"\n",
    "    Pad a list of 1D integer tensors to the same length.\n",
    "\n",
    "    Requirements:\n",
    "    - Return PaddedBatch(tokens, lengths, padding_mask)\n",
    "    - padding_mask[b, t] == True iff t >= lengths[b]\n",
    "    - tokens should be dtype long, if not cast them\n",
    "    \"\"\"\n",
    "    batch_size = len(seqs)\n",
    "    lengths = torch.tensor([len(seq) for seq in seqs], dtype=torch.long)\n",
    "    max_length = lengths.max().item()\n",
    "\n",
    "    # Initialize padded tokens and padding mask\n",
    "    padded_tokens = torch.full((batch_size, max_length), pad_value, dtype=torch.long)\n",
    "    padding_mask = torch.ones((batch_size, max_length), dtype=torch.bool)\n",
    "\n",
    "    for i, seq in enumerate(seqs):\n",
    "        seq_len = lengths[i].item()\n",
    "        padded_tokens[i, :seq_len] = seq\n",
    "        padding_mask[i, :seq_len] = False  # Mark non-padding positions\n",
    "\n",
    "    return PaddedBatch(tokens=padded_tokens, lengths=lengths, padding_mask=padding_mask)\n",
    "\n",
    "# Test pad_1d_sequences\n",
    "seqs_test = [torch.tensor([1,2,3]), torch.tensor([4,5]), torch.tensor([6])]\n",
    "padded_batch = pad_1d_sequences(seqs_test, pad_value=0)\n",
    "print(\"Testing pad_1d_sequences:\")\n",
    "print(f\"Tokens:\\n{padded_batch.tokens}\")\n",
    "print(f\"Lengths:\\n{padded_batch.lengths}\")\n",
    "print(f\"Padding Mask:\\n{padded_batch.padding_mask}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2df849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing collate_next_token_batch:\n",
      "Input IDs:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 0],\n",
      "        [6, 0, 0]])\n",
      "Target IDs:\n",
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 0],\n",
      "        [7, 0, 0]])\n",
      "Attention Mask:\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True,  True, False],\n",
      "        [ True, False, False]])\n",
      "Padding Mask:\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "def collate_next_token_batch(\n",
    "    batch: list[tuple[torch.Tensor, torch.Tensor]], pad_value: int = 0\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate for NextTokenDataset samples that may have variable lengths.\n",
    "\n",
    "    batch: list of (input_ids, target_ids), each 1D\n",
    "\n",
    "    Return dict with:\n",
    "      - input_ids: (B, T_max)\n",
    "      - target_ids: (B, T_max)\n",
    "      - attention_mask: (B, T_max) where True means \"keep\" (NOT padding)\n",
    "      - padding_mask: (B, T_max) where True means \"padding\"\n",
    "\n",
    "    Requirements:\n",
    "    - pad input_ids and target_ids consistently\n",
    "    - attention_mask is the logical NOT of padding_mask\n",
    "    \"\"\"\n",
    "    # Separate input_ids and target_ids from the batch\n",
    "    input_ids_list = [item[0] for item in batch]\n",
    "    target_ids_list = [item[1] for item in batch]\n",
    "\n",
    "    # Pad both input_ids and target_ids using the pad_1d_sequences function\n",
    "    padded_inputs = pad_1d_sequences(input_ids_list, pad_value=pad_value)\n",
    "    padded_targets = pad_1d_sequences(target_ids_list, pad_value=pad_value)\n",
    "\n",
    "    # Create attention_mask as the logical NOT of padding_mask\n",
    "    attention_mask = ~padded_inputs.padding_mask\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_inputs.tokens,\n",
    "        \"target_ids\": padded_targets.tokens,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"padding_mask\": padded_inputs.padding_mask,\n",
    "    }\n",
    "\n",
    "# Test collate_next_token_batch\n",
    "batch_test = [\n",
    "    (torch.tensor([1,2,3]), torch.tensor([2,3,4])),\n",
    "    (torch.tensor([4,5]), torch.tensor([5,6])),\n",
    "    (torch.tensor([6]), torch.tensor([7])),\n",
    "]\n",
    "collated = collate_next_token_batch(batch_test, pad_value=0)\n",
    "print(\"Testing collate_next_token_batch:\")\n",
    "print(f\"Input IDs:\\n{collated['input_ids']}\")\n",
    "print(f\"Target IDs:\\n{collated['target_ids']}\")\n",
    "print(f\"Attention Mask:\\n{collated['attention_mask']}\")\n",
    "print(f\"Padding Mask:\\n{collated['padding_mask']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bdd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing make_dataloader with NextTokenDataset:\n",
      "{'input_ids': tensor([[1, 2, 3],\n",
      "        [5, 6, 0]]), 'target_ids': tensor([[2, 3, 4],\n",
      "        [6, 7, 0]]), 'attention_mask': tensor([[ True,  True,  True],\n",
      "        [ True,  True, False]]), 'padding_mask': tensor([[False, False, False],\n",
      "        [False, False,  True]])}\n",
      "{'input_ids': tensor([[8]]), 'target_ids': tensor([[9]]), 'attention_mask': tensor([[True]]), 'padding_mask': tensor([[False]])}\n"
     ]
    }
   ],
   "source": [
    "def make_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = False,\n",
    "    collate_fn=None,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader with optional collate_fn.\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "# Test make_dataloader with NextTokenDataset and collate_next_token_batch\n",
    "# Create a list of 1D tensors\n",
    "tokens_test = [torch.tensor([1,2,3,4]), torch.tensor([5,6,7]), torch.tensor([8,9])]\n",
    "dataset = NextTokenDataset(tokens_test)\n",
    "dataloader = make_dataloader(dataset, batch_size=2, shuffle=False, collate_fn=collate_next_token_batch)\n",
    "print(\"Testing make_dataloader with NextTokenDataset:\")\n",
    "for batch in dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab920b",
   "metadata": {},
   "source": [
    "## Optimizers (AdamW from scratch)\n",
    "\n",
    "PyTorch optimizers keep **state** for each parameter (e.g. moment estimates in Adam).\n",
    "In this section you’ll implement **AdamW**, which is Adam + *decoupled* weight decay.\n",
    "\n",
    "### AdamW state\n",
    "For each parameter tensor `p` we store:\n",
    "- `m`: first moment (EMA of gradients)\n",
    "- `v`: second moment (EMA of squared gradients)\n",
    "- `t`: step counter\n",
    "\n",
    "### Update overview (high level)\n",
    "1) Update moments `m, v`\n",
    "2) Bias-correct them (`m_hat, v_hat`)\n",
    "3) Apply parameter update:\n",
    "   `p -= lr * ( m_hat / (sqrt(v_hat) + eps) + weight_decay * p )`\n",
    "\n",
    "Notes:\n",
    "- This update is **in-place** (mutates `p`).\n",
    "- Gradients should not be modified.\n",
    "- State tensors must match parameter shape/device/dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4c124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "from this import d\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AdamWState:\n",
    "    \"\"\"\n",
    "    Per-parameter AdamW state.\n",
    "\n",
    "    m: first moment\n",
    "    v: second moment\n",
    "    t: step count\n",
    "    \"\"\"\n",
    "\n",
    "    m: torch.Tensor\n",
    "    v: torch.Tensor\n",
    "    t: int\n",
    "\n",
    "\n",
    "def init_adamw_state(p: torch.Tensor) -> AdamWState:\n",
    "    \"\"\"\n",
    "    Initialize AdamW state tensors for a parameter tensor p.\n",
    "\n",
    "    What to create:\n",
    "    - m: zeros like p, same shape/device/dtype\n",
    "    - v: zeros like p, same shape/device/dtype\n",
    "    - t: step counter starting at 0\n",
    "\n",
    "    Notes / requirements:\n",
    "    - Use torch.zeros_like(p) for m and v.\n",
    "    - Do NOT attach gradients to the state (initialize under torch.no_grad()).\n",
    "    - t starts at 0. In adamw_step_, increment t to 1 on the first update *before*\n",
    "      computing bias correction terms (1 - beta1^t) and (1 - beta2^t).\n",
    "    - State tensors must live on the same device as p (CPU vs GPU) and have the\n",
    "      same dtype as p.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        m = torch.zeros_like(p, device=p.device, dtype=p.dtype)\n",
    "        v = torch.zeros_like(p, device=p.device, dtype=p.dtype)\n",
    "    t = 0\n",
    "    return AdamWState(m=m, v=v, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7eeaa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step_(\n",
    "    p: torch.Tensor,\n",
    "    grad: torch.Tensor,\n",
    "    state: AdamWState,\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> AdamWState:\n",
    "    \"\"\"\n",
    "    In-place AdamW parameter update (updates p).\n",
    "\n",
    "    Algorithm (AdamW):\n",
    "      m = beta1*m + (1-beta1)*grad\n",
    "      v = beta2*v + (1-beta2)*grad^2\n",
    "      m_hat = m / (1 - beta1^t)\n",
    "      v_hat = v / (1 - beta2^t)\n",
    "      p = p - lr * (m_hat / (sqrt(v_hat) + eps) + weight_decay * p)\n",
    "\n",
    "    Requirements:\n",
    "    - Update p in-place.\n",
    "    - Return updated state (with incremented t).\n",
    "    - Do not modify grad.\n",
    "    - Should work for any tensor shape.\n",
    "    \"\"\"\n",
    "    t= state.t + 1  # Increment step count\n",
    "    m = state.m\n",
    "    v = state.v\n",
    "    beta1, beta2 = betas\n",
    "\n",
    "    # Update first moment\n",
    "    m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "    # Update second moment\n",
    "    v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "    # Compute bias-corrected moments\n",
    "    m_hat = m / (1 - beta1 ** t)\n",
    "    v_hat = v / (1 - beta2 ** t)\n",
    "\n",
    "    # Update parameters in-place\n",
    "    p -= lr * (m_hat / (torch.sqrt(v_hat) + eps) + weight_decay * p)\n",
    "\n",
    "    return AdamWState(m=m, v=v, t=t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bf5bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step_many_(\n",
    "    params: list[torch.Tensor],\n",
    "    grads: list[torch.Tensor],\n",
    "    states: list[AdamWState],\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> list[AdamWState]:\n",
    "    \"\"\"\n",
    "    Apply AdamW to many parameters.\n",
    "\n",
    "    Requirements:\n",
    "    - len(params) == len(grads) == len(states)\n",
    "    - Update each param in-place.\n",
    "    - Return the list of updated states.\n",
    "    \"\"\"\n",
    "    updated_states = []\n",
    "    for p, grad, state in zip(params, grads, states):\n",
    "        updated_state = adamw_step_(p, grad, state, lr, betas, eps, weight_decay)\n",
    "        updated_states.append(updated_state)\n",
    "    return updated_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b571bd",
   "metadata": {},
   "source": [
    "## Training basics\n",
    "\n",
    "A minimal training step follows the same pattern almost everywhere:\n",
    "\n",
    "1) set model to train mode\n",
    "2) reset gradients\n",
    "3) forward pass\n",
    "4) compute loss\n",
    "5) backward pass\n",
    "6) step optimizer\n",
    "\n",
    "In this exercise you’ll implement a single MSE training step using a standard PyTorch optimizer.\n",
    "Return a Python float loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24c815a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_mse(\n",
    "    model: nn.Module,\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    One MSE train step using standard torch optimizer.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    inputs, targets = batch\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = nn.functional.mse_loss(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb996eb7",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "\n",
    "Initialization matters because it controls signal and gradient scales at the start of training.\n",
    "\n",
    "### Fan-in / fan-out\n",
    "- `fan_in`: number of input connections to a unit\n",
    "- `fan_out`: number of output connections from a unit\n",
    "\n",
    "For a Linear layer weight of shape `(out_features, in_features)`:\n",
    "- `fan_in = in_features`\n",
    "- `fan_out = out_features`\n",
    "\n",
    "### Common schemes\n",
    "- **Xavier / Glorot** (often good for tanh / linear-ish nets):\n",
    "  keeps variance stable across layers when activations are roughly symmetric.\n",
    "- **Kaiming / He** (often good for ReLU-like nets):\n",
    "  accounts for the fact that ReLU zeroes out about half the inputs.\n",
    "\n",
    "In this section you’ll implement Xavier uniform and Kaiming uniform and use them to initialize `nn.Linear`.\n",
    "We also always zero the bias unless explicitly told otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c34eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fan_in_fan_out:\n",
      "Weight shape: torch.Size([64, 128])\n",
      "Fan-in: 128\n",
      "Fan-out: 64\n"
     ]
    }
   ],
   "source": [
    "def fan_in_fan_out(weight: torch.Tensor) -> tuple[int, int]:\n",
    "    \"\"\"Compute (fan_in, fan_out) for a weight tensor.\"\"\"\n",
    "    if weight.ndim < 2:\n",
    "        raise ValueError(\"Weight tensor must have at least 2 dimensions\")\n",
    "    fan_in = weight.size(1) * weight[0][0].numel()  # Product of dimensions except the first\n",
    "    fan_out = weight.size(0) * weight[0][0].numel()  # Product of dimensions except the second\n",
    "    return fan_in, fan_out\n",
    "\n",
    "# Test fan_in_fan_out\n",
    "weight_test = torch.randn(64, 128)  # Example weight tensor\n",
    "fan_in, fan_out = fan_in_fan_out(weight_test)\n",
    "print(\"Testing fan_in_fan_out:\")\n",
    "print(f\"Weight shape: {weight_test.shape}\")\n",
    "print(f\"Fan-in: {fan_in}\")\n",
    "print(f\"Fan-out: {fan_out}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d421c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing xavier_uniform_:\n",
      "Weight shape: torch.Size([64, 128])\n",
      "Weight stats: mean=0.0004, std=0.1027\n"
     ]
    }
   ],
   "source": [
    "def xavier_uniform_(weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Xavier/Glorot uniform init:\n",
    "      bound = gain * sqrt(6 / (fan_in + fan_out))\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = fan_in_fan_out(weight)\n",
    "    bound = gain * (6 / (fan_in + fan_out)) ** 0.5\n",
    "    with torch.no_grad():\n",
    "        return weight.uniform_(-bound, bound)\n",
    "\n",
    "# Test xavier_uniform_\n",
    "weight_test = torch.empty(64, 128)  # Example weight tensor\n",
    "xavier_uniform_(weight_test, gain=1.0)\n",
    "print(\"Testing xavier_uniform_:\")\n",
    "print(f\"Weight shape: {weight_test.shape}\")\n",
    "print(f\"Weight stats: mean={weight_test.mean().item():.4f}, std={weight_test.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e59d69d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kaiming_uniform_:\n",
      "Weight shape: torch.Size([64, 128])\n",
      "Weight stats: mean=0.0014, std=0.1247\n"
     ]
    }
   ],
   "source": [
    "def kaiming_uniform_(weight: torch.Tensor, nonlinearity: str = \"relu\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Kaiming/He uniform init.\n",
    "\n",
    "    Follow this common choice:\n",
    "      gain = sqrt(2) for ReLU\n",
    "      std = gain / sqrt(fan_in)\n",
    "      bound = sqrt(3) * std\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = fan_in_fan_out(weight)\n",
    "    gain = 2 ** 0.5 if nonlinearity == \"relu\" else 1.0\n",
    "    std = gain / (fan_in ** 0.5)\n",
    "    bound = (3 ** 0.5) * std\n",
    "    with torch.no_grad():\n",
    "        return weight.uniform_(-bound, bound)\n",
    "\n",
    "# Test kaiming_uniform_\n",
    "weight_test = torch.empty(64, 128)  # Example weight tensor\n",
    "kaiming_uniform_(weight_test, nonlinearity= \"relu\")\n",
    "print(\"Testing kaiming_uniform_:\")\n",
    "print(f\"Weight shape: {weight_test.shape}\")\n",
    "print(f\"Weight stats: mean={weight_test.mean().item():.4f}, std={weight_test.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af69be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing init_linear_ with Xavier:\n",
      "Weight stats: mean=-0.0003, std=0.1028\n",
      "Testing init_linear_ with Kaiming ReLU:\n",
      "Weight stats: mean=0.0025, std=0.1245\n",
      "Testing init_linear_ with Zero:\n",
      "Weight stats: mean=0.0000, std=0.0000\n"
     ]
    }
   ],
   "source": [
    "def init_linear_(layer: nn.Linear, scheme: str = \"xavier\") -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Initialize an nn.Linear in-place.\n",
    "\n",
    "    scheme:\n",
    "      - \"xavier\"\n",
    "      - \"kaiming_relu\"\n",
    "      - \"zero\" (weights and bias = 0)\n",
    "    \"\"\"\n",
    "    if scheme == \"xavier\":\n",
    "        xavier_uniform_(layer.weight, gain=1.0)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    elif scheme == \"kaiming_relu\":\n",
    "        kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    elif scheme == \"zero\":\n",
    "        nn.init.zeros_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization scheme: {scheme}\")\n",
    "    return layer\n",
    "\n",
    "# Test init_linear_\n",
    "linear_test = nn.Linear(128, 64)\n",
    "init_linear_(linear_test, scheme=\"xavier\")\n",
    "print(\"Testing init_linear_ with Xavier:\")\n",
    "print(f\"Weight stats: mean={linear_test.weight.mean().item():.4f}, std={linear_test.weight.std().item():.4f}\")\n",
    "init_linear_(linear_test, scheme=\"kaiming_relu\")\n",
    "print(\"Testing init_linear_ with Kaiming ReLU:\")\n",
    "print(f\"Weight stats: mean={linear_test.weight.mean().item():.4f}, std={linear_test.weight.std().item():.4f}\")\n",
    "init_linear_(linear_test, scheme=\"zero\")\n",
    "print(\"Testing init_linear_ with Zero:\")\n",
    "print(f\"Weight stats: mean={linear_test.weight.mean().item():.4f}, std={linear_test.weight.std().item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
